{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sirishasubbu/FMML-COURSE-ASSINMENTS/blob/main/Module_01_Lab_02_MLPractice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Eu9VZbF01eq"
      },
      "source": [
        "# Machine learning terms and metrics\n",
        "\n",
        "FMML Module 1, Lab 2<br>\n",
        "\n",
        "\n",
        " In this lab, we will show a part of the ML pipeline by extracting features, training and testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qBvyEem0vLi"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "# set randomseed\n",
        "rng = np.random.default_rng(seed=42)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3t59g5s1HfC"
      },
      "source": [
        "In this lab, we will use the California Housing dataset. There are 20640 samples, each with 8 attributes like income of the block, age of the houses per district etc. The task is to predict the cost of the houses per district.\n",
        "\n",
        "Let us download and examine the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LpqjN991GGJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dab8c64-68d8-46cb-841e-aed87df16f55"
      },
      "source": [
        " dataset =  datasets.fetch_california_housing()\n",
        " # print(dataset.DESCR)  # uncomment this if you want to know more about this dataset\n",
        " # print(dataset.keys())  # if you want to know what else is there in this dataset\n",
        " dataset.target = dataset.target.astype(np.int) # so that we can classify\n",
        " print(dataset.data.shape)\n",
        " print(dataset.target.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20640, 8)\n",
            "(20640,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-60ae2e9a125e>:4: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  dataset.target = dataset.target.astype(np.int) # so that we can classify\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNx4174W5xRg"
      },
      "source": [
        "Here is a function for calculating the 1-nearest neighbours"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07zpydQj1hIQ"
      },
      "source": [
        "def NN1(traindata, trainlabel, query):\n",
        "  diff  = traindata - query  # find the difference between features. Numpy automatically takes care of the size here\n",
        "  sq = diff*diff # square the differences\n",
        "  dist = sq.sum(1) # add up the squares\n",
        "  label = trainlabel[np.argmin(dist)] # our predicted label is the label of the training data which has the least distance from the query\n",
        "  return label\n",
        "\n",
        "def NN(traindata, trainlabel, testdata):\n",
        "  # we will run nearest neighbour for each sample in the test data\n",
        "  # and collect the predicted classes in an array using list comprehension\n",
        "  predlabel = np.array([NN1(traindata, trainlabel, i) for i in testdata])\n",
        "  return predlabel"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03JktkfIGaje"
      },
      "source": [
        "We will also define a 'random classifier', which randomly allots labels to each sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fogWAtjyGhAH"
      },
      "source": [
        "def RandomClassifier(traindata, trainlabel, testdata):\n",
        "  # in reality, we don't need these arguments\n",
        "\n",
        "  classes = np.unique(trainlabel)\n",
        "  rints = rng.integers(low=0, high=len(classes), size=len(testdata))\n",
        "  predlabel = classes[rints]\n",
        "  return predlabel"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hjf1KHs7fU5"
      },
      "source": [
        "Let us define a metric 'Accuracy' to see how good our learning algorithm is. Accuracy is the ratio of the number of correctly classified samples to the total number of samples. The higher the accuracy, the better the algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouuCqWU07bz-"
      },
      "source": [
        "def Accuracy(gtlabel, predlabel):\n",
        "  assert len(gtlabel)==len(predlabel), \"Length of the groundtruth labels and predicted labels should be the same\"\n",
        "  correct = (gtlabel==predlabel).sum() # count the number of times the groundtruth label is equal to the predicted label.\n",
        "  return correct/len(gtlabel)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vJFwBFa9Klw"
      },
      "source": [
        "Let us make a function to split the dataset with the desired probability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ko0VzpSM2Tdi"
      },
      "source": [
        "def split(data, label, percent):\n",
        "  # generate a random number for each sample\n",
        "  rnd = rng.random(len(label))\n",
        "  split1 = rnd<percent\n",
        "  split2 = rnd>=percent\n",
        "  split1data = data[split1,:]\n",
        "  split1label = label[split1]\n",
        "  split2data = data[split2,:]\n",
        "  split2label = label[split2]\n",
        "  return split1data, split1label, split2data, split2label"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcK3LEAJ_LGC"
      },
      "source": [
        "We will reserve 20% of our dataset as the test set. We will not change this portion throughout our experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBZkHBLJ1iU-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3c6a0b3-cec8-4cda-a618-c8951e7ab11d"
      },
      "source": [
        "testdata, testlabel, alltraindata, alltrainlabel = split(dataset.data, dataset.target, 20/100)\n",
        "print('Number of test samples = ', len(testlabel))\n",
        "print('Number of other samples = ', len(alltrainlabel))\n",
        "print('Percent of test data = ', len(testlabel)*100/len(dataset.target),'%')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of test samples =  4144\n",
            "Number of other samples =  16496\n",
            "Percent of test data =  20.07751937984496 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6Ss0Z6IAGNV"
      },
      "source": [
        "## Experiments with splits\n",
        "\n",
        "Let us reserve some of our train data as a validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFew2iry_7W7"
      },
      "source": [
        "traindata, trainlabel, valdata, vallabel = split(alltraindata, alltrainlabel, 75/100)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60hiu4clFN1i"
      },
      "source": [
        "What is the accuracy of our classifiers on the train dataset?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBlZDTHUFTZx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59aef3a2-e11c-42c5-ce0d-6b9b5fc9aed7"
      },
      "source": [
        "trainpred = NN(traindata, trainlabel, traindata)\n",
        "trainAccuracy = Accuracy(trainlabel, trainpred)\n",
        "print(\"Train accuracy using nearest neighbour is \", trainAccuracy)\n",
        "\n",
        "trainpred = RandomClassifier(traindata, trainlabel, traindata)\n",
        "trainAccuracy = Accuracy(trainlabel, trainpred)\n",
        "print(\"Train accuracy using random classifier is \", trainAccuracy)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy using nearest neighbour is  1.0\n",
            "Train accuracy using random classifier is  0.164375808538163\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7h08-9gJDtSy"
      },
      "source": [
        "For nearest neighbour, the train accuracy is always 1. The accuracy of the random classifier is close to 1/(number of classes) which is 0.1666 in our case.\n",
        "\n",
        "Let us predict the labels for our validation set and get the accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h7bXoW_2H3v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8b6e855-d288-4f12-b178-a09d4e0f9b51"
      },
      "source": [
        "valpred = NN(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy using nearest neighbour is \", valAccuracy)\n",
        "\n",
        "valpred = RandomClassifier(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy using random classifier is \", valAccuracy)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy using nearest neighbour is  0.34108527131782945\n",
            "Validation accuracy using random classifier is  0.1688468992248062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py9bLguFEjfg"
      },
      "source": [
        "Validation accuracy of nearest neighbour is considerably less than its train accuracy while the validation accuracy of random classifier is the same. However, the validation accuracy of nearest neighbour is twice that of the random classifier.\n",
        "\n",
        "Now let us try another random split and check the validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujm3cyYzEntE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f9e2935-43e0-4dce-d218-89bccbaec776"
      },
      "source": [
        "traindata, trainlabel, valdata, vallabel = split(alltraindata, alltrainlabel, 75/100)\n",
        "valpred = NN(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy of nearest neighbour is \", valAccuracy)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy of nearest neighbour is  0.34048257372654156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSOx7U83EKie"
      },
      "source": [
        "You can run the above cell multiple times to try with different random splits.\n",
        "We notice that the accuracy is different for each run, but close together.\n",
        "\n",
        "Now let us compare it with the accuracy we get on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNEZ5ToYBEDW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd048a47-5099-4bdc-cb8e-a7b7f363d0fa"
      },
      "source": [
        "testpred = NN(alltraindata, alltrainlabel, testdata)\n",
        "testAccuracy = Accuracy(testlabel, testpred)\n",
        "print('Test accuracy is ', testAccuracy)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy is  0.34917953667953666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3dGD531K3gH"
      },
      "source": [
        "### Try it out for yourself and answer:\n",
        "1. How is the accuracy of the validation set affected if we increase the percentage of validation set? What happens when we reduce it?\n",
        "2. How does the size of the train and validation set affect how well we can predict the accuracy on the test set using the validation set?\n",
        "3. What do you think is a good percentage to reserve for the validation set so that thest two factors are balanced?\n",
        "\n",
        "Answer for both nearest neighbour and random classifier. You can note down the values for your experiments and plot a graph using  <a href=https://matplotlib.org/stable/gallery/lines_bars_and_markers/step_demo.html#sphx-glr-gallery-lines-bars-and-markers-step-demo-py>plt.plot<href>. Check also for extreme values for splits, like 99.9% or 0.1%"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1a**:Larger Training Set:\n",
        "\n",
        "If you allocate a larger portion of your data to the training set, your model has more data to learn from. This can lead to a model that generalizes better, as it has been exposed to a wider range of examples. However, if the validation set is relatively small, it might not be representative enough to reliably estimate your model's performance.\n",
        "\n",
        "Larger Validation Set:\n",
        "\n",
        "A larger validation set provides a more reliable estimate of your model's performance because it is evaluated on a larger, more representative portion of your data. However, if the training set becomes too small due to a large validation set, it could lead to overfitting, where the model fits the training data too closely but doesn't generalize well to new, unseen data.\n",
        "\n",
        "Balanced Split:\n",
        "\n",
        "Ideally, you want a balanced split that allocates a sufficient amount of data to both the training and validation sets. Common splits include 70-30, 80-20, or 90-10 for training-validation ratios. In summary, finding the right balance between the sizes of the training and validation sets is crucial. which is typically evaluated using a separate test set. Cross-validation techniques can also be used to assess model performance more reliably, especially when dealing with limited d\n"
      ],
      "metadata": {
        "id": "RzF6iHcRyidc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2a**:import numpy as np import matplotlib.pyplot as plt from sklearn.neighbors import KNeighborsClassifier from sklearn.dummy import DummyClassifier from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "9lyklom50qH6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3a**: The percentage to reserve for the validation set, while balancing the need for model training and reliable performance estimation, depends on several factors, including the size of your dataset and the complexity of your model. However, a common practice is to allocate around 20-30% of your dataset to the validation set.\n",
        "\n",
        "Here are some considerations:\n",
        "\n",
        "Small Datasets: If you have a relatively small dataset (e.g., hundreds or a few thousand data points), you might lean towards a larger validation set, such as 20-30%, to ensure a more representative sample for performance estimation.\n",
        "\n",
        "Large Datasets: If your dataset is quite large (e.g., tens of thousands or more data points), you can allocate a smaller percentage to the validation set, like 10-20%, because there's already a substantial amount of data available for training.\n",
        "\n",
        "Complex Models: If you're training a highly complex model, you might want a larger validation set to ensure that your model isn't overfitting. Complex models have more capacity to fit the training data closely, so a larger validation set can help detect overfitting.\n",
        "\n",
        "Simple Models: For simpler models with lower capacity, you might be able to get away with a slightly smaller validation set since they are less prone to overfitting. Remember that these percentages are not strict rules but general guidelines. You should also consider using techniques like k-fold cross-validation if you have limited data. Cross-validation involves splitting your data into multiple folds, repeatedly training and validating your model on different subsets, and then averaging the results. This provides a more robust estimate of your model's performance.\n"
      ],
      "metadata": {
        "id": "eufAR1TS0wbv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnYvkAZLQY7h"
      },
      "source": [
        "## Multiple Splits\n",
        "\n",
        "One way to get more accurate estimates for the test accuracy is by using <b>crossvalidation</b>. Here, we will try a simple version, where we do multiple train/val splits and take the average of validation accuracies as the test accuracy estimation. Here is a function for doing this. Note that this function will take a long time to execute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4nGCUQXBTzo"
      },
      "source": [
        "# you can use this function for random classifier also\n",
        "def AverageAccuracy(alldata, alllabel, splitpercent, iterations, classifier=NN):\n",
        "  accuracy = 0\n",
        "  for ii in range(iterations):\n",
        "    traindata, trainlabel, valdata, vallabel = split(alldata, alllabel, splitpercent)\n",
        "    valpred = classifier(traindata, trainlabel, valdata)\n",
        "    accuracy += Accuracy(vallabel, valpred)\n",
        "  return accuracy/iterations # average of all accuracies"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3qtNar7Bbik",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f05065cb-6819-4835-b93f-1907e100910c"
      },
      "source": [
        "print('Average validation accuracy is ', AverageAccuracy(alltraindata, alltrainlabel, 75/100, 10, classifier=NN))\n",
        "testpred = NN(alltraindata, alltrainlabel, testdata)\n",
        "print('test accuracy is ',Accuracy(testlabel, testpred) )"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average validation accuracy is  0.33584635395170215\n",
            "test accuracy is  0.34917953667953666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33GIn4x5VH-d"
      },
      "source": [
        "This is a very simple way of doing cross-validation. There are many well-known algorithms for cross-validation, like k-fold cross-validation, leave-one-out etc. This will be covered in detail in a later module. For more information about cross-validation, check <a href=https://en.wikipedia.org/wiki/Cross-validation_(statistics)>Cross-validatioin (Wikipedia)</a>\n",
        "\n",
        "### Questions\n",
        "1. Does averaging the validation accuracy across multiple splits give more consistent results?\n",
        "2. Does it give more accurate estimate of test accuracy?\n",
        "3. What is the effect of the number of iterations on the estimate? Do we get a better estimate with higher iterations?\n",
        "4. Consider the results you got for the previous questions. Can we deal with a very small train dataset or validation dataset by increasing the iterations?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1a**: Yes, averaging the validation accuracy across multiple splits of your data (using techniques like k-fold cross-validation) can provide more consistent and reliable results compared to a single split. This is because it helps mitigate the impact of data variability and ensures that your model's performance estimate is more robust. Here's why it's beneficial:\n",
        "\n",
        "Reduced Variability: By repeatedly splitting your data into different subsets for training and validation, you reduce the impact of random data variations. A single split can lead to overfitting or underestimating model performance due to the specific subset used. Averaging over multiple splits smooths out these variations.\n",
        "\n",
        "Better Generalization: Cross-validation allows your model to be evaluated on different parts of the data, making it better at generalizing to unseen data. This provides a more realistic estimate of how your model will perform in real-world scenarios.\n",
        "\n",
        "More Comprehensive Evaluation: Cross-validation assesses your model's performance across various data configurations. It's particularly useful when your dataset is limited or unbalanced because it ensures that each data point is used for validation at least once.\n",
        "\n",
        "Model Selection: When comparing different models or hyperparameters, cross-validation provides a fair basis for comparison. It helps you choose the model that performs consistently well across different data splits"
      ],
      "metadata": {
        "id": "RvqlAe4Aw8fS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2a**:Cross-validation, such as k-fold cross-validation, provides a more accurate estimation of test accuracy compared to a single validation split. While it's important to note that cross-validation doesn't directly estimate the accuracy on a completely unseen test dataset (as the test dataset is meant to be entirely separate), it does provide a more reliable approximation of how your model is likely to perform on new, unseen data. Here's why:\n",
        "\n",
        "Reduced Overfitting or Luck Influence: With a single validation split, your model's performance estimate can be heavily influenced by the specific subset of data chosen for validation. This can lead to overfitting to that particular subset or, conversely, underestimating the model's performance due to bad luck in the selection of the validation set.\n",
        "\n",
        "Comprehensive Assessment: Cross-validation assesses your model's performance across multiple, different validation sets. This provides a more comprehensive understanding of how your model is likely to perform on new data.\n",
        "\n",
        "Model Selection: Cross-validation is often used not only for performance estimation but also for model selection and hyperparameter tuning. It helps identify models that consistently perform well across different data splits, which is a valuable indicator of how a model will perform on unseen data.\n"
      ],
      "metadata": {
        "id": "3H4HNsVfxxhu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3a**:In the context of machine learning, the number of iterations can have an impact on the estimate of model performance when using techniques like k-fold cross-validation or bootstrapping. Generally, increasing the number of iterations tends to provide a more stable and reliable estimate of model performance. Here's why:\n",
        "\n",
        "Reduced Variability: With more iterations, the estimate of performance becomes less sensitive to the specific random splits of the data. Each iteration represents a different subset of data for training and validation, and increasing the number of iterations reduces the influence of randomness or the choice of a specific subset.\n",
        "\n",
        "Smoothing Effects: More iterations allow for a smoother and more representative distribution of performance scores. Averaging performance across a larger number of iterations helps smooth out any outliers or extreme values that may occur in a smaller number of iterations.\n",
        "\n",
        "Better Confidence: As you increase the number of iterations, you gain greater confidence in the stability of your performance estimate. You can be more certain that the estimate is representative of your model's true performance.\n",
        "\n",
        "However, there are practical trade-offs to consider when choosing the number of iterations:\n",
        "\n",
        "Computational Cost: Increasing the number of iterations requires more computational resources and time. You should balance the computational cost with the benefits of a more accurate estimate.\n",
        "\n",
        "Diminishing Returns: There may be diminishing returns in terms of accuracy gains as you increase the number of iterations. At some point, the added computational cost may not significantly improve the estimate's stability.\n",
        "\n",
        "In practice, common choices for the number of iterations in cross-validation are 5-fold, 10-fold, or even leave-one-out cross-validation (where each data point is treated as a separate validation set).\n"
      ],
      "metadata": {
        "id": "jr6gwVgYx7od"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4a**:Increasing the number of iterations in techniques like k-fold cross-validation can help compensate for having a smaller training or validation dataset to some extent. However, there are limitations to how much you can mitigate the effects of small datasets solely by increasing\n",
        "\n",
        "iterations:\n",
        "\n",
        "Training Dataset Size:\n",
        "\n",
        "Increasing the number of iterations primarily helps with the stability and reliability of the performance estimate. However, if your training dataset is extremely small, even with more iterations, the model may not have sufficient data to learn meaningful patterns, especially for complex models. In such cases, the model might underperform or overfit.\n",
        "\n",
        "Validation Dataset Size:\n",
        "\n",
        "A larger validation dataset obtained through more iterations can provide more reliable estimates of model performance. This can lead to inaccurate estimates of model performance, as the validation set may not adequately cover the diversity of the data.\n",
        "\n",
        "Trade-offs:\n",
        "\n",
        "Increasing the number of iterations comes with computational costs. The time and resources required for training and evaluating the model multiple times can become significant. In summary, while increasing iterations in cross-validation can help improve the reliability of performance estimates when dealing with small training or validation datasets, it's not a substitute for having a reasonably sized dataset. If you have very limited data, you should consider strategies like data augmentation, transfer learning, or collecting more data if possible to improve your model's performance and the reliability of performance estimates."
      ],
      "metadata": {
        "id": "sPmJC5gvyKf4"
      }
    }
  ]
}